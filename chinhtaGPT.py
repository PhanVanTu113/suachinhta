# spellcheck_web.py
import streamlit as st
import os
from openai import OpenAI
from io import BytesIO
import base64
import fitz  # PyMuPDF
import textwrap
from docx import Document
from difflib import ndiff

openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("ğŸš« Há»‡ thá»‘ng chÆ°a cáº¥u hÃ¬nh API Key. Vui lÃ²ng liÃªn há»‡ quáº£n trá»‹ viÃªn Ä‘á»ƒ thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng OPENAI_API_KEY.")
    st.stop()

client = OpenAI(api_key=openai_api_key)

st.set_page_config(
    page_title="Trá»£ lÃ½ Sá»­a lá»—i ChÃ­nh táº£ | ECOVIS AFA VIETNAM",
    page_icon="ğŸ“",
    layout="wide",
)

password = st.text_input("ğŸ” Nháº­p máº­t kháº©u Ä‘á»ƒ truy cáº­p:", type="password")
if password != "ecovis2025":
    st.warning("Vui lÃ²ng nháº­p Ä‘Ãºng máº­t kháº©u.")
    st.stop()

st.markdown("""
<style>
.result-box {
    padding: 1rem;
    border: 1px solid #ccc;
    border-radius: 10px;
    background-color: #f9f9f9;
    white-space: pre-wrap;
    font-family: monospace;
}
.changed {
    background-color: #ffffcc;
}
.download-btn {
    margin-top: 1rem;
}
</style>
""", unsafe_allow_html=True)

col1, col2 = st.columns([1, 10])
with col1:
    st.image("LOGO ECOVIS AFA VIETNAM.jpg", width=80)
with col2:
    st.title("ğŸ“˜ Trá»£ lÃ½ Sá»­a lá»—i ChÃ­nh táº£ | ECOVIS AFA VIETNAM")

st.markdown("---")

st.markdown("""
#### ğŸ“‚ HÆ°á»›ng dáº«n sá»­ dá»¥ng:
1. Táº£i lÃªn tá»‡p vÄƒn báº£n tiáº¿ng Viá»‡t cáº§n kiá»ƒm tra lá»—i chÃ­nh táº£ (há»— trá»£ Ä‘á»‹nh dáº¡ng `.txt`, `.docx`, `.pdf`).
2. Há»‡ thá»‘ng sáº½ kiá»ƒm tra vÃ  hiá»ƒn thá»‹ káº¿t quáº£ Ä‘Ã£ chá»‰nh sá»­a ngay bÃªn dÆ°á»›i.
3. CÃ³ thá»ƒ táº£i káº¿t quáº£ vá» dÆ°á»›i dáº¡ng `.txt` hoáº·c `.docx` Ä‘á»ƒ lÆ°u trá»¯.
4. So sÃ¡nh Ä‘oáº¡n vÄƒn gá»‘c vÃ  Ä‘oáº¡n Ä‘Ã£ sá»­a Ä‘á»ƒ tháº¥y rÃµ thay Ä‘á»•i.
5. CÃ³ thá»ƒ lá»±a chá»n mÃ´ hÃ¬nh GPT-3.5 Ä‘á»ƒ tiáº¿t kiá»‡m chi phÃ­.
---
""")

model = st.radio("ğŸ” Chá»n mÃ´ hÃ¬nh AI Ä‘á»ƒ kiá»ƒm tra:", ["gpt-3.5-turbo", "gpt-4o"], index=1)

uploaded_file = st.file_uploader("ğŸ“„ Táº£i lÃªn tá»‡p cáº§n kiá»ƒm tra chÃ­nh táº£:", type=["txt", "docx", "pdf"])

if uploaded_file:
    with st.spinner("ğŸ“‘ Äang xá»­ lÃ½ tá»‡p..."):
        file_text = ""
        paragraphs = []
        doc = None

        if uploaded_file.type == "text/plain":
            file_text = uploaded_file.read().decode("utf-8", errors="ignore")
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            doc = Document(uploaded_file)
            for para in doc.paragraphs:
                text = para.text.strip()
                if text:
                    paragraphs.append((para, text))
                    file_text += text + "\n"
        elif uploaded_file.type == "application/pdf":
            pdf_doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
            for page in pdf_doc:
                file_text += page.get_text()

        if not file_text.strip():
            st.warning("ğŸ“­ KhÃ´ng thá»ƒ Ä‘á»c Ä‘Æ°á»£c ná»™i dung tá»« tá»‡p Ä‘Ã£ táº£i lÃªn.")
            st.stop()

        def chunk_text(text, max_length=3000):
            paragraphs = textwrap.wrap(text, width=max_length, break_long_words=False, replace_whitespace=False)
            return paragraphs

        corrected_all = ""
        original_all = ""
        total_tokens = 0

        def highlight_diff(original, corrected):
            diff = ndiff(original.split(), corrected.split())
            highlighted = []
            for word in diff:
                if word.startswith('+'):
                    highlighted.append(f"<span class='changed'>{word[2:]}</span>")
                elif word.startswith('-'):
                    continue
                elif word.startswith(' '):
                    highlighted.append(word[2:])
            return ' '.join(highlighted)

        if doc and paragraphs:
            highlighted_output = []
            for para, text in paragraphs:
                res = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "Báº¡n lÃ  má»™t chuyÃªn gia ngÃ´n ngá»¯ tiáº¿ng Viá»‡t. HÃ£y sá»­a lá»—i chÃ­nh táº£ trong Ä‘oáº¡n vÄƒn sau."},
                        {"role": "user", "content": text}
                    ],
                    temperature=0.3,
                    max_tokens=1024
                )
                corrected = res.choices[0].message.content.strip()
                para.text = corrected
                corrected_all += corrected + "\n"
                original_all += text + "\n"
                if hasattr(res, "usage"):
                    total_tokens += res.usage.total_tokens
                highlighted_output.append(highlight_diff(text, corrected))
        else:
            chunks = chunk_text(file_text)
            highlighted_output = []
            for chunk in chunks:
                res = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "Báº¡n lÃ  má»™t chuyÃªn gia ngÃ´n ngá»¯ tiáº¿ng Viá»‡t. HÃ£y sá»­a lá»—i chÃ­nh táº£ trong Ä‘oáº¡n vÄƒn sau."},
                        {"role": "user", "content": chunk}
                    ],
                    temperature=0.3,
                    max_tokens=1024
                )
                corrected = res.choices[0].message.content.strip()
                corrected_all += corrected + "\n"
                original_all += chunk + "\n"
                if hasattr(res, "usage"):
                    total_tokens += res.usage.total_tokens
                highlighted_output.append(highlight_diff(chunk, corrected))

        st.subheader("ğŸ“ So sÃ¡nh vÄƒn báº£n trÆ°á»›c vÃ  sau khi sá»­a lá»—i")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**ğŸ“„ VÄƒn báº£n gá»‘c:**")
            st.text_area("", original_all[:5000], height=300)
        with col2:
            st.markdown("**âœ… ÄÃ£ sá»­a lá»—i (bÃ´i vÃ ng chá»— thay Ä‘á»•i):**")
            st.markdown(f"<div class='result-box'>{'<br>'.join(highlighted_output)}</div>", unsafe_allow_html=True)

        if total_tokens > 0:
            st.info(f"ğŸ”¢ Token Ä‘Ã£ sá»­ dá»¥ng: {total_tokens} (Æ°á»›c tÃ­nh chi phÃ­ ~{total_tokens / 1000 * (0.01 if model == 'gpt-3.5-turbo' else 0.03):.4f} USD)")

        if doc:
            output = BytesIO()
            doc.save(output)
            b64_docx = base64.b64encode(output.getvalue()).decode()
            st.markdown(f'<a class="download-btn" href="data:application/octet-stream;base64,{b64_docx}" download="ket_qua_da_sua.docx">ğŸ“¥ Táº£i file Word Ä‘Ã£ sá»­a</a>', unsafe_allow_html=True)
        else:
            b64 = base64.b64encode(corrected_all.encode()).decode()
            href = f'<a class="download-btn" href="data:file/txt;base64,{b64}" download="ket_qua_da_sua.txt">ğŸ“¥ Táº£i káº¿t quáº£ vá»</a>'
            st.markdown(href, unsafe_allow_html=True)

        st.markdown("---")
        if st.button("ğŸ”„ Báº¯t Ä‘áº§u phiÃªn kiá»ƒm tra má»›i"):
            st.experimental_rerun()

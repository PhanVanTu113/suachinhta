# spellcheck_web.py
import streamlit as st
import os
from openai import OpenAI
from io import BytesIO
import base64
import fitz  # PyMuPDF
from docx import Document
from difflib import ndiff, SequenceMatcher
import re
import time

openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("ğŸš« Há»‡ thá»‘ng chÆ°a cáº¥u hÃ¬nh API Key. Vui lÃ²ng liÃªn há»‡ quáº£n trá»‹ viÃªn Ä‘á»ƒ thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng OPENAI_API_KEY.")
    st.stop()

client = OpenAI(api_key=openai_api_key)

st.set_page_config(
    page_title="Trá»£ lÃ½ Sá»­a lá»—i ChÃ­nh táº£ | ECOVIS AFA VIETNAM",
    page_icon="ğŸ“",
    layout="wide",
)

password = st.text_input("ğŸ” Nháº­p máº­t kháº©u Ä‘á»ƒ truy cáº­p:", type="password")
if password != "ecovis2025":
    st.warning("Vui lÃ²ng nháº­p Ä‘Ãºng máº­t kháº©u.")
    st.stop()

st.markdown("""
<style>
.result-box {
    padding: 1rem;
    border: 1px solid #ccc;
    border-radius: 10px;
    background-color: #f9f9f9;
    white-space: pre-wrap;
    font-family: monospace;
}
.changed {
    background-color: #ffffcc;
}
.download-btn {
    margin-top: 1rem;
}
</style>
""", unsafe_allow_html=True)

col1, col2 = st.columns([1, 10])
with col1:
    st.image("LOGO ECOVIS AFA VIETNAM.jpg", width=80)
with col2:
    st.title("ğŸ“˜ Trá»£ lÃ½ Sá»­a lá»—i ChÃ­nh táº£ | ECOVIS AFA VIETNAM")

st.markdown("---")

st.markdown("""
#### ğŸ“‚ HÆ°á»›ng dáº«n sá»­ dá»¥ng:
1. Táº£i lÃªn tá»‡p vÄƒn báº£n tiáº¿ng Viá»‡t cáº§n kiá»ƒm tra lá»—i chÃ­nh táº£ (há»— trá»£ Ä‘á»‹nh dáº¡ng `.txt`, `.docx`, `.pdf`).
2. Há»‡ thá»‘ng sáº½ kiá»ƒm tra vÃ  hiá»ƒn thá»‹ káº¿t quáº£ Ä‘Ã£ chá»‰nh sá»­a ngay bÃªn dÆ°á»›i.
3. CÃ³ thá»ƒ táº£i káº¿t quáº£ vá» dÆ°á»›i dáº¡ng `.txt`, `.docx` hoáº·c `.pdf` Ä‘á»ƒ lÆ°u trá»¯.
4. So sÃ¡nh Ä‘oáº¡n vÄƒn gá»‘c vÃ  Ä‘oáº¡n Ä‘Ã£ sá»­a Ä‘á»ƒ tháº¥y rÃµ thay Ä‘á»•i.
5. CÃ³ thá»ƒ lá»±a chá»n mÃ´ hÃ¬nh GPT-3.5 Ä‘á»ƒ tiáº¿t kiá»‡m chi phÃ­.
---
""")

model = st.radio("ğŸ” Chá»n mÃ´ hÃ¬nh AI Ä‘á»ƒ kiá»ƒm tra:", ["gpt-3.5-turbo", "gpt-4o"], index=1)

uploaded_file = st.file_uploader("ğŸ“„ Táº£i lÃªn tá»‡p cáº§n kiá»ƒm tra chÃ­nh táº£:", type=["txt", "docx", "pdf"])

if uploaded_file:
    with st.spinner("ğŸ“‘ Äang xá»­ lÃ½ tá»‡p..."):
        file_text = ""
        paragraphs = []
        doc = None
        pdf_data = None
        filename = uploaded_file.name.rsplit('.', 1)[0]

        if uploaded_file.type == "text/plain":
            file_text = uploaded_file.read().decode("utf-8", errors="ignore")
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            doc = Document(uploaded_file)
            for para in doc.paragraphs:
                text = para.text.strip()
                if text:
                    paragraphs.append((para, text))
                    file_text += text + "\n"
        elif uploaded_file.type == "application/pdf":
            pdf_data = uploaded_file.read()
            pdf_doc = fitz.open(stream=pdf_data, filetype="pdf")
            for page in pdf_doc:
                lines = page.get_text("text").split("\n")
                for line in lines:
                    if line.strip() and len(line.strip()) > 10:
                        file_text += line.strip() + "\n"

        if not file_text.strip():
            st.warning("ğŸ“­ KhÃ´ng thá»ƒ Ä‘á»c Ä‘Æ°á»£c ná»™i dung tá»« tá»‡p Ä‘Ã£ táº£i lÃªn.")
            st.stop()

        def chunk_sentences(text, max_len=500):
            sentences = re.split(r'(?<=[.!?])\s+', text)
            chunks = []
            chunk = ""
            for s in sentences:
                if len(chunk) + len(s) < max_len:
                    chunk += s + " "
                else:
                    chunks.append(chunk.strip())
                    chunk = s + " "
            if chunk:
                chunks.append(chunk.strip())
            return chunks

        def highlight_diff(original, corrected):
            diff = ndiff(original.split(), corrected.split())
            highlighted = []
            for word in diff:
                if word.startswith('+'):
                    highlighted.append(f"<span class='changed'>{word[2:]}</span>")
                elif word.startswith('-'):
                    continue
                elif word.startswith(' '):
                    highlighted.append(word[2:])
            return ' '.join(highlighted)

        def count_corrections(orig, corrected):
            sm = SequenceMatcher(None, orig.split(), corrected.split())
            return sum(1 for tag, _, _, _, _ in sm.get_opcodes() if tag != 'equal')

        corrected_all = ""
        original_all = ""
        highlighted_output = []
        total_tokens = 0
        total_corrections = 0
        start_time = time.time()

        if doc and paragraphs:
            for para, text in paragraphs:
                try:
                    res = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": "Báº¡n lÃ  má»™t chuyÃªn gia ngÃ´n ngá»¯ tiáº¿ng Viá»‡t. HÃ£y sá»­a lá»—i chÃ­nh táº£ trong Ä‘oáº¡n vÄƒn sau."},
                            {"role": "user", "content": text}
                        ],
                        temperature=0.3,
                        max_tokens=1024
                    )
                    corrected = res.choices[0].message.content.strip()
                    para.text = corrected
                    corrected_all += corrected + "\n"
                    original_all += text + "\n"
                    total_corrections += count_corrections(text, corrected)
                    if hasattr(res, "usage"):
                        total_tokens += res.usage.total_tokens
                    highlighted_output.append(highlight_diff(text, corrected))
                except Exception as e:
                    highlighted_output.append(f"<span style='color:red;'>Lá»—i xá»­ lÃ½ Ä‘oáº¡n: {text[:50]}... âœ {str(e)}</span>")
        else:
            chunks = chunk_sentences(file_text)
            for chunk in chunks:
                try:
                    res = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": "Báº¡n lÃ  má»™t chuyÃªn gia ngÃ´n ngá»¯ tiáº¿ng Viá»‡t. HÃ£y sá»­a lá»—i chÃ­nh táº£ trong Ä‘oáº¡n vÄƒn sau."},
                            {"role": "user", "content": chunk}
                        ],
                        temperature=0.3,
                        max_tokens=1024
                    )
                    corrected = res.choices[0].message.content.strip()
                    corrected_all += corrected + "\n"
                    original_all += chunk + "\n"
                    total_corrections += count_corrections(chunk, corrected)
                    if hasattr(res, "usage"):
                        total_tokens += res.usage.total_tokens
                    highlighted_output.append(highlight_diff(chunk, corrected))
                except Exception as e:
                    highlighted_output.append(f"<span style='color:red;'>Lá»—i xá»­ lÃ½ Ä‘oáº¡n: {chunk[:50]}... âœ {str(e)}</span>")

        duration = time.time() - start_time

        st.subheader("ğŸ“ So sÃ¡nh vÄƒn báº£n trÆ°á»›c vÃ  sau khi sá»­a lá»—i")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**ğŸ“„ VÄƒn báº£n gá»‘c:**")
            st.text_area("", original_all[:5000], height=300)
        with col2:
            st.markdown("**âœ… ÄÃ£ sá»­a lá»—i (bÃ´i vÃ ng chá»— thay Ä‘á»•i):**")
            st.markdown(f"<div class='result-box'>{'<br>'.join(highlighted_output)}</div>", unsafe_allow_html=True)

        st.info(f"ğŸ”¢ Token Ä‘Ã£ sá»­ dá»¥ng: {total_tokens} (Æ°á»›c tÃ­nh chi phÃ­ ~{total_tokens / 1000 * (0.01 if model == 'gpt-3.5-turbo' else 0.03):.4f} USD)")
        st.info(f"âœï¸ Tá»•ng sá»‘ lá»—i Ä‘Ã£ sá»­a: {total_corrections}")
        st.info(f"â±ï¸ Thá»i gian xá»­ lÃ½: {duration:.2f} giÃ¢y")

        if doc:
            output = BytesIO()
            doc.save(output)
            b64_docx = base64.b64encode(output.getvalue()).decode()
            st.markdown(f'<a class="download-btn" href="data:application/octet-stream;base64,{b64_docx}" download="{filename}_da_sua.docx">ğŸ“¥ Táº£i file Word Ä‘Ã£ sá»­a</a>', unsafe_allow_html=True)
        elif pdf_data:
            from reportlab.pdfgen import canvas
            from reportlab.lib.pagesizes import letter
            from PyPDF2 import PdfWriter, PdfReader
            import tempfile

            # Táº¡o PDF má»›i tá»« ná»™i dung Ä‘Ã£ sá»­a
            temp_pdf = BytesIO()
            c = canvas.Canvas(temp_pdf, pagesize=letter)
            textobject = c.beginText(40, 750)
            for line in corrected_all.split("
"):
                textobject.textLine(line)
            c.drawText(textobject)
            c.save()

            # Táº¡o file káº¿t há»£p náº¿u muá»‘n ghÃ©p láº¡i ná»n cÅ© + sá»­a má»›i (tuá»³ chá»‰nh nÃ¢ng cao)
            # CÃ²n khÃ´ng thÃ¬ chá»‰ cáº§n xuáº¥t file sá»­a
            temp_pdf.seek(0)
            b64_pdf_corrected = base64.b64encode(temp_pdf.read()).decode()
            st.markdown(f'<a class="download-btn" href="data:application/pdf;base64,{b64_pdf_corrected}" download="{filename}_da_sua.pdf">ğŸ“¥ Táº£i file PDF Ä‘Ã£ sá»­a</a>', unsafe_allow_html=True)">ğŸ“¥ Táº£i láº¡i file PDF gá»‘c</a>', unsafe_allow_html=True)
        else:
            b64 = base64.b64encode(corrected_all.encode()).decode()
            href = f'<a class="download-btn" href="data:file/txt;base64,{b64}" download="{filename}_da_sua.txt">ğŸ“¥ Táº£i káº¿t quáº£ vá»</a>'
            st.markdown(href, unsafe_allow_html=True)

        st.markdown("---")
        if st.button("ğŸ”„ Báº¯t Ä‘áº§u phiÃªn kiá»ƒm tra má»›i"):
            preserved_model = st.session_state.get("model", "gpt-4o")
            keys_to_clear = list(st.session_state.keys())
            for key in keys_to_clear:
                del st.session_state[key]
            st.session_state["model"] = preserved_model
            st.experimental_rerun()

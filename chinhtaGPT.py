# spellcheck_web.py
import streamlit as st
import os
from openai import OpenAI
from io import BytesIO
import base64
import fitz  # PyMuPDF
from docx import Document
from difflib import ndiff, SequenceMatcher
import re
import time

openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("üö´ H·ªá th·ªëng ch∆∞a c·∫•u h√¨nh API Key. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n ƒë·ªÉ thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng OPENAI_API_KEY.")
    st.stop()

client = OpenAI(api_key=openai_api_key)

st.set_page_config(
    page_title="Tr·ª£ l√Ω S·ª≠a l·ªói Ch√≠nh t·∫£ | ECOVIS AFA VIETNAM",
    page_icon="üìù",
    layout="wide",
)

password = st.text_input("üîê Nh·∫≠p m·∫≠t kh·∫©u ƒë·ªÉ truy c·∫≠p:", type="password")
if password != "ecovis2025":
    st.warning("Vui l√≤ng nh·∫≠p ƒë√∫ng m·∫≠t kh·∫©u.")
    st.stop()

st.markdown("""
<style>
.result-box {
    padding: 1rem;
    border: 1px solid #ccc;
    border-radius: 10px;
    background-color: #f9f9f9;
    white-space: pre-wrap;
    font-family: monospace;
}
.changed {
    background-color: #ffffcc;
}
.download-btn {
    margin-top: 1rem;
}
</style>
""", unsafe_allow_html=True)

col1, col2 = st.columns([1, 10])
with col1:
    st.image("LOGO ECOVIS AFA VIETNAM.jpg", width=80)
with col2:
    st.title("üìò Tr·ª£ l√Ω S·ª≠a l·ªói Ch√≠nh t·∫£ | ECOVIS AFA VIETNAM")

st.markdown("---")

st.markdown("""
#### üìÇ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:
1. T·∫£i l√™n t·ªáp vƒÉn b·∫£n ti·∫øng Vi·ªát c·∫ßn ki·ªÉm tra l·ªói ch√≠nh t·∫£ (h·ªó tr·ª£ ƒë·ªãnh d·∫°ng `.txt`, `.docx`, `.pdf`).
2. H·ªá th·ªëng s·∫Ω ki·ªÉm tra v√† hi·ªÉn th·ªã k·∫øt qu·∫£ ƒë√£ ch·ªânh s·ª≠a ngay b√™n d∆∞·ªõi.
3. C√≥ th·ªÉ t·∫£i k·∫øt qu·∫£ v·ªÅ d∆∞·ªõi d·∫°ng `.txt`, `.docx` ho·∫∑c `.pdf` ƒë·ªÉ l∆∞u tr·ªØ.
4. So s√°nh ƒëo·∫°n vƒÉn g·ªëc v√† ƒëo·∫°n ƒë√£ s·ª≠a ƒë·ªÉ th·∫•y r√µ thay ƒë·ªïi.
5. C√≥ th·ªÉ l·ª±a ch·ªçn m√¥ h√¨nh GPT-3.5 ƒë·ªÉ ti·∫øt ki·ªám chi ph√≠.
---
""")

model = st.radio("üîç Ch·ªçn m√¥ h√¨nh AI ƒë·ªÉ ki·ªÉm tra:", ["gpt-3.5-turbo", "gpt-4o"], index=1)

uploaded_file = st.file_uploader("üìÑ T·∫£i l√™n t·ªáp c·∫ßn ki·ªÉm tra ch√≠nh t·∫£:", type=["txt", "docx", "pdf"])

if uploaded_file:
    with st.spinner("üìë ƒêang x·ª≠ l√Ω t·ªáp..."):
        file_text = ""
        paragraphs = []
        doc = None
        pdf_data = None
        filename = uploaded_file.name.rsplit('.', 1)[0]

        if uploaded_file.type == "text/plain":
            file_text = uploaded_file.read().decode("utf-8", errors="ignore")
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            doc = Document(uploaded_file)
            for para in doc.paragraphs:
                text = para.text.strip()
                if text:
                    paragraphs.append((para, text))
                    file_text += text + "\n"
        elif uploaded_file.type == "application/pdf":
            pdf_data = uploaded_file.read()
            pdf_doc = fitz.open(stream=pdf_data, filetype="pdf")
            for page in pdf_doc:
                lines = page.get_text("text").split("\n")
                for line in lines:
                    if line.strip() and len(line.strip()) > 10:
                        file_text += line.strip() + "\n"

        if not file_text.strip():
            st.warning("üì≠ Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung t·ª´ t·ªáp ƒë√£ t·∫£i l√™n.")
            st.stop()

        def chunk_sentences(text, max_len=500):
            sentences = re.split(r'(?<=[.!?])\s+', text)
            chunks = []
            chunk = ""
            for s in sentences:
                if len(chunk) + len(s) < max_len:
                    chunk += s + " "
                else:
                    chunks.append(chunk.strip())
                    chunk = s + " "
            if chunk:
                chunks.append(chunk.strip())
            return chunks

        def highlight_diff(original, corrected):
            diff = ndiff(original.split(), corrected.split())
            highlighted = []
            for word in diff:
                if word.startswith('+'):
                    highlighted.append(f"<span class='changed'>{word[2:]}</span>")
                elif word.startswith('-'):
                    continue
                elif word.startswith(' '):
                    highlighted.append(word[2:])
            return ' '.join(highlighted)

        def count_corrections(orig, corrected):
            sm = SequenceMatcher(None, orig.split(), corrected.split())
            return sum(1 for tag, _, _, _, _ in sm.get_opcodes() if tag != 'equal')

        corrected_all = ""
        original_all = ""
        highlighted_output = []
        total_tokens = 0
        total_corrections = 0
        start_time = time.time()

        if doc and paragraphs:
            for para, text in paragraphs:
                try:
                    res = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": "B·∫°n l√† m·ªôt chuy√™n gia ng√¥n ng·ªØ ti·∫øng Vi·ªát. H√£y s·ª≠a l·ªói ch√≠nh t·∫£ trong ƒëo·∫°n vƒÉn sau."},
                            {"role": "user", "content": text}
                        ],
                        temperature=0.3,
                        max_tokens=1024
                    )
                    corrected = res.choices[0].message.content.strip()
                    para.text = corrected
                    corrected_all += corrected + "\n"
                    original_all += text + "\n"
                    total_corrections += count_corrections(text, corrected)
                    if hasattr(res, "usage"):
                        total_tokens += res.usage.total_tokens
                    highlighted_output.append(highlight_diff(text, corrected))
                except Exception as e:
                    highlighted_output.append(f"<span style='color:red;'>L·ªói x·ª≠ l√Ω ƒëo·∫°n: {text[:50]}... ‚ûú {str(e)}</span>")
        else:
            chunks = chunk_sentences(file_text)
            for chunk in chunks:
                try:
                    res = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": "B·∫°n l√† m·ªôt chuy√™n gia ng√¥n ng·ªØ ti·∫øng Vi·ªát. H√£y s·ª≠a l·ªói ch√≠nh t·∫£ trong ƒëo·∫°n vƒÉn sau."},
                            {"role": "user", "content": chunk}
                        ],
                        temperature=0.3,
                        max_tokens=1024
                    )
                    corrected = res.choices[0].message.content.strip()
                    corrected_all += corrected + "\n"
                    original_all += chunk + "\n"
                    total_corrections += count_corrections(chunk, corrected)
                    if hasattr(res, "usage"):
                        total_tokens += res.usage.total_tokens
                    highlighted_output.append(highlight_diff(chunk, corrected))
                except Exception as e:
                    highlighted_output.append(f"<span style='color:red;'>L·ªói x·ª≠ l√Ω ƒëo·∫°n: {chunk[:50]}... ‚ûú {str(e)}</span>")

        duration = time.time() - start_time

        st.subheader("üìù So s√°nh vƒÉn b·∫£n tr∆∞·ªõc v√† sau khi s·ª≠a l·ªói")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**üìÑ VƒÉn b·∫£n g·ªëc:**")
            st.text_area("", original_all[:5000], height=300)
        with col2:
            st.markdown("**‚úÖ ƒê√£ s·ª≠a l·ªói (b√¥i v√†ng ch·ªó thay ƒë·ªïi):**")
            st.markdown(f"<div class='result-box'>{'<br>'.join(highlighted_output)}</div>", unsafe_allow_html=True)

        st.info(f"üî¢ Token ƒë√£ s·ª≠ d·ª•ng: {total_tokens} (∆∞·ªõc t√≠nh chi ph√≠ ~{total_tokens / 1000 * (0.01 if model == 'gpt-3.5-turbo' else 0.03):.4f} USD)")
        st.info(f"‚úèÔ∏è T·ªïng s·ªë l·ªói ƒë√£ s·ª≠a: {total_corrections}")
        st.info(f"‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω: {duration:.2f} gi√¢y")

        if doc:
            output = BytesIO()
            doc.save(output)
            b64_docx = base64.b64encode(output.getvalue()).decode()
            st.markdown(f'<a class="download-btn" href="data:application/octet-stream;base64,{b64_docx}" download="{filename}_da_sua.docx">üì• T·∫£i file Word ƒë√£ s·ª≠a</a>', unsafe_allow_html=True)
        elif pdf_data:
            from reportlab.pdfgen import canvas
            from reportlab.lib.pagesizes import letter
            from PyPDF2 import PdfWriter, PdfReader
            import tempfile

            # T·∫°o PDF m·ªõi t·ª´ n·ªôi dung ƒë√£ s·ª≠a
            temp_pdf = BytesIO()
            c = canvas.Canvas(temp_pdf, pagesize=letter)
            textobject = c.beginText(40, 750)
            for line in corrected_all.split("
"):
                textobject.textLine(line)
            c.drawText(textobject)
            c.save()

            # T·∫°o file k·∫øt h·ª£p n·∫øu mu·ªën gh√©p l·∫°i n·ªÅn c≈© + s·ª≠a m·ªõi (tu·ª≥ ch·ªânh n√¢ng cao)
            # C√≤n kh√¥ng th√¨ ch·ªâ c·∫ßn xu·∫•t file s·ª≠a
            temp_pdf.seek(0)
            b64_pdf_corrected = base64.b64encode(temp_pdf.read()).decode()
            st.markdown(f'<a class="download-btn" href="data:application/pdf;base64,{b64_pdf_corrected}" download="{filename}_da_sua.pdf">üì• T·∫£i file PDF ƒë√£ s·ª≠a</a>', unsafe_allow_html=True)">üì• T·∫£i l·∫°i file PDF g·ªëc</a>', unsafe_allow_html=True)
        else:
            b64 = base64.b64encode(corrected_all.encode()).decode()
            href = f'<a class="download-btn" href="data:file/txt;base64,{b64}" download="{filename}_da_sua.txt">üì• T·∫£i k·∫øt qu·∫£ v·ªÅ</a>'
            st.markdown(href, unsafe_allow_html=True)

        st.markdown("---")
        if st.button("üîÑ B·∫Øt ƒë·∫ßu phi√™n ki·ªÉm tra m·ªõi"):
            preserved_model = st.session_state.get("model", "gpt-4o")
            keys_to_clear = list(st.session_state.keys())
            for key in keys_to_clear:
                del st.session_state[key]
            st.session_state["model"] = preserved_model
            st.experimental_rerun()
